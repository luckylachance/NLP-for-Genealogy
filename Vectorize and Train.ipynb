{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  Iterate through htm files and strip html and save as a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "path = 'data/'\n",
    "arr = os.listdir(path)\n",
    "for filename in arr:\n",
    "    if filename.endswith('.txt'):\n",
    "        xpath = os.path.join(path,filename)\n",
    "        with open(xpath, 'r') as f:\n",
    "                text = f.read()\n",
    "                reclean = re.compile('<.*?>')\n",
    "                htmlclean = re.sub(reclean,'', text)\n",
    "                cleantext = \" \".join(htmlclean.split())\n",
    "                outFile = open(path + os.path.splitext(filename)[0] +\".txt\",'w')\n",
    "                outFile.write(cleantext)\n",
    "                outFile.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single File test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English # Import the English language class\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#nlp = English()\n",
    "#nlp = spacy.laod('en')\n",
    "\n",
    "path = 'data/'\n",
    "arr = os.listdir(path)\n",
    "for filename in arr:\n",
    "    if filename.endswith('.txt'):\n",
    "        xpath = os.path.join(path,filename)\n",
    "        with open(xpath, 'r') as file:\n",
    "            contents = file.read()\n",
    "            doc = nlp(contents)\n",
    "            entities = [(ent.label_, ent.text, filename) for ent in doc.ents]\n",
    "            df = pd.DataFrame(entities, columns =['type', 'text', 'filename']) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>SEC</td>\n",
       "      <td>424B5_d844216d424b5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DATE</td>\n",
       "      <td>July 5, 2017</td>\n",
       "      <td>424B5_d844216d424b5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPE</td>\n",
       "      <td>Japan</td>\n",
       "      <td>424B5_d844216d424b5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORG</td>\n",
       "      <td>the Japan Bank for International Cooperation Act</td>\n",
       "      <td>424B5_d844216d424b5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PERCENT</td>\n",
       "      <td>1.750%</td>\n",
       "      <td>424B5_d844216d424b5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>6,913 6,910</td>\n",
       "      <td>424B5_d844216d424b5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>ORG</td>\n",
       "      <td>Education</td>\n",
       "      <td>424B5_d844216d424b5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>ORG</td>\n",
       "      <td>Science</td>\n",
       "      <td>424B5_d844216d424b5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>6,145 5,846</td>\n",
       "      <td>424B5_d844216d424b5.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>5,598 5,703</td>\n",
       "      <td>424B5_d844216d424b5.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>805 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         type                                              text  \\\n",
       "0         ORG                                               SEC   \n",
       "1        DATE                                      July 5, 2017   \n",
       "2         GPE                                             Japan   \n",
       "3         ORG  the Japan Bank for International Cooperation Act   \n",
       "4     PERCENT                                            1.750%   \n",
       "..        ...                                               ...   \n",
       "800  CARDINAL                                       6,913 6,910   \n",
       "801       ORG                                         Education   \n",
       "802       ORG                                           Science   \n",
       "803  CARDINAL                                       6,145 5,846   \n",
       "804  CARDINAL                                       5,598 5,703   \n",
       "\n",
       "                    filename  \n",
       "0    424B5_d844216d424b5.txt  \n",
       "1    424B5_d844216d424b5.txt  \n",
       "2    424B5_d844216d424b5.txt  \n",
       "3    424B5_d844216d424b5.txt  \n",
       "4    424B5_d844216d424b5.txt  \n",
       "..                       ...  \n",
       "800  424B5_d844216d424b5.txt  \n",
       "801  424B5_d844216d424b5.txt  \n",
       "802  424B5_d844216d424b5.txt  \n",
       "803  424B5_d844216d424b5.txt  \n",
       "804  424B5_d844216d424b5.txt  \n",
       "\n",
       "[805 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[(15374025435414067454, 125, 127)]\n",
      "[(15374025435414067454, 127, 129)]\n",
      "[(15374025435414067454, 127, 129)]\n",
      "[(15374025435414067454, 228, 230)]\n",
      "[(15374025435414067454, 137, 139)]\n",
      "[(15374025435414067454, 137, 139)]\n",
      "[(15374025435414067454, 247, 249), (15374025435414067454, 722, 724), (15374025435414067454, 1440, 1442)]\n",
      "[(15374025435414067454, 2110, 2112), (15374025435414067454, 2361, 2363)]\n",
      "[(15374025435414067454, 185, 187)]\n",
      "[(15374025435414067454, 6305, 6307)]\n",
      "[(15374025435414067454, 2110, 2112), (15374025435414067454, 2361, 2363)]\n",
      "[(15374025435414067454, 5194, 5196), (15374025435414067454, 7826, 7828)]\n",
      "[(15374025435414067454, 111, 113)]\n",
      "[(15374025435414067454, 772, 774), (15374025435414067454, 14148, 14150), (15374025435414067454, 14501, 14503), (15374025435414067454, 14537, 14539)]\n",
      "[(15374025435414067454, 771, 773)]\n",
      "[(15374025435414067454, 768, 770), (15374025435414067454, 14028, 14030), (15374025435414067454, 14377, 14379), (15374025435414067454, 14413, 14415)]\n",
      "[(15374025435414067454, 1384, 1386)]\n",
      "[(15374025435414067454, 4142, 4144), (15374025435414067454, 4244, 4246), (15374025435414067454, 4372, 4374)]\n",
      "[(15374025435414067454, 2378, 2380), (15374025435414067454, 3497, 3499)]\n",
      "[(15374025435414067454, 247, 249), (15374025435414067454, 700, 702), (15374025435414067454, 1300, 1302)]\n",
      "[(15374025435414067454, 4564, 4566)]\n",
      "[(15374025435414067454, 322, 324), (15374025435414067454, 741, 743), (15374025435414067454, 1326, 1328)]\n",
      "[(15374025435414067454, 1058, 1060)]\n",
      "[(15374025435414067454, 5011, 5013), (15374025435414067454, 5766, 5768), (15374025435414067454, 10591, 10593), (15374025435414067454, 25243, 25245), (15374025435414067454, 31712, 31714)]\n",
      "[(15374025435414067454, 3446, 3448)]\n",
      "[(15374025435414067454, 6560, 6562), (15374025435414067454, 6730, 6732), (15374025435414067454, 9263, 9265), (15374025435414067454, 12749, 12751)]\n",
      "[(15374025435414067454, 5091, 5093)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English # Import the English language class\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#nlp = English()\n",
    "#nlp = spacy.laod('en')\n",
    "\n",
    "path = 'data/'\n",
    "arr = os.listdir(path)\n",
    "for filename in arr:\n",
    "    if filename.endswith('.txt'):\n",
    "        xpath = os.path.join(path,filename)\n",
    "        with open(xpath, 'r') as file:\n",
    "            contents = file.read()\n",
    "            matcher = Matcher(nlp.vocab)\n",
    "            pattern = [{'LOWER': 'cusip'},{}]\n",
    "            matcher.add(\"CUSIP\", None, pattern)\n",
    "            doc = nlp(contents)\n",
    "            matches = matcher(doc)\n",
    "            print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Regex to extract CUSIP and Issuer to an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = 'data/'\n",
    "files=[i for i in os.listdir(path) if i.endswith(\"txt\")]\n",
    "\n",
    "w={'FILE':[],'CUSIP': [], 'ISSUER': []}\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    f=open(path+ file)\n",
    "    data=f.read()\n",
    "    a=re.findall(r'[\\n]?[_-]?\\s+(?:[_-]{9,})?[\\s\\r\\t\\n]*\\(CUSIP Number\\)',data)\n",
    "    a1=\"\".join(a).replace('\\n',' ')\n",
    "    b=re.findall(r'Issuer(.*?)issuer',data,re.DOTALL)\n",
    "    b1=\"\".join(b).replace('\\n',' ')\n",
    "     #b=re.findall(r'CUSIP(.*?)cusip',data,re.DOTALL)\n",
    "\n",
    "    loc= os.path.join(file)\n",
    "    \n",
    "    w['FILE'].append(loc)\n",
    "    w['CUSIP'].append(a1)\n",
    "    w['ISSUER'].append(b1) \n",
    "    #print (os.path.join(file))\n",
    "\n",
    "df=pd.DataFrame(data=w)\n",
    "df.to_excel('data/return.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NLTK to assess poatterns of words associated with tagged list for Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPROACH:  The problem could be viewed as a supervised learning problem where I assist by tagging the callables and then train them recognize callable using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Lucky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "path = 'data/'\n",
    "arr = os.listdir(path)\n",
    "data = []\n",
    "for filename in arr:\n",
    "    if filename.endswith('.txt'):\n",
    "        xpath = os.path.join(path,filename)\n",
    "        with open(xpath, 'r') as file:\n",
    "            contents = file.read()\n",
    "            data.extend(tokenizer.tokenize(contents))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns =['text']) \n",
    "df.to_excel('data/sentences.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Such factors may have the effect of discouragi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If 3-month U.S. dollar LIBOR is discontinued o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The notes differ from conventional fixed-rate ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Validity of the Notes   In the opinion of Davi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“NY Federal Reserve’s website” means the websi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Such factors may have the effect of discouragi...      0\n",
       "1  If 3-month U.S. dollar LIBOR is discontinued o...      0\n",
       "2  The notes differ from conventional fixed-rate ...      0\n",
       "3  Validity of the Notes   In the opinion of Davi...      0\n",
       "4  “NY Federal Reserve’s website” means the websi...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data\\callable_text.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(849, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape #Only 849 total sentences evaluated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 849 entries, 0 to 848\n",
      "Data columns (total 2 columns):\n",
      "text     849 non-null object\n",
      "label    849 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 13.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()  # marker was manually marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    754\n",
       "1     95\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts() # 1 indicates text associated with callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokening the data using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English # Import the English language class\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation# Create our list of punctuation marks\n",
    "nlp = spacy.load('en') # Create list of stopwords\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "parser = English()# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "\n",
    "\n",
    "def spacy_tokenizer(sentence):# Creating tokenizer function\n",
    "    \n",
    "    mytokens = parser(sentence)# Creating token object, used to create documents with linguistic annotations.\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ] # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ] # Removing stop words\n",
    "    \n",
    "    return mytokens# return preprocessed list of tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['text'] # the features we want to analyze\n",
    "ylabels = data['label'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucky\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x000001D3CD3021D0>),\n",
       "                ('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x000001D3C86E2950>,\n",
       "                                 vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9490196078431372\n",
      "Logistic Regression Precision: 0.9545454545454546\n",
      "Logistic Regression Recall: 0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, overall, our model correctly identified sentence sentiment 94.9% of the time. When it predicted a \"callible\", that classification was actually callable 96% of the time. When handed an unseen callable sentence, our model identified it as positive 68.4% of the time (just ok -- better than a coin flip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Build a dataframe from the files in the data folder and add a callable column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>callable</th>\n",
       "      <th>filename</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>We may, at our option, redeem the floating rat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>424B2_d859033d424b2_N.txt</td>\n",
       "      <td>We may redeem any series of the notes at our o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>424B2_dp114411_424b2-mtn1197_N.txt</td>\n",
       "      <td>CALCULATION OF REGISTRATION FEE Title of each ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>424B2_dp115966_424b2-255_N.txt</td>\n",
       "      <td>Citigroup Global Markets Holdings Inc. Novembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>424B2_dp116691_424b2-257_N.txt</td>\n",
       "      <td>Citigroup Global Markets Holdings Inc. Novembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>424B2_dp120183_424b2-302_N.txt</td>\n",
       "      <td>Citigroup Global Markets Holdings Inc. January...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>424B2_g110203424b2_N.txt</td>\n",
       "      <td>Royal Bank of Canada Filed Pursuant to Rule 42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>424B2_gs-424b2_N.txt</td>\n",
       "      <td>gs-424b2.htm Filed Pursuant to Rule 424(b)(2) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>424B3_pricingsupplement2-yrfrn_N.txt</td>\n",
       "      <td>Document CALCULATION OF REGISTRATION FEE Title...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>424B5_d832846d424b5_C.txt</td>\n",
       "      <td>If at any time the Bank shall become obligated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>424B5_d834717d424b5_C.txt</td>\n",
       "      <td>We may redeem the notes at our option, in whol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>Unless we redeem the notes earlier, the notes ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   callable                              filename  \\\n",
       "0                       424B2_d833231d424b2_C.txt   \n",
       "1                       424B2_d859033d424b2_N.txt   \n",
       "2              424B2_dp114411_424b2-mtn1197_N.txt   \n",
       "3                  424B2_dp115966_424b2-255_N.txt   \n",
       "4                  424B2_dp116691_424b2-257_N.txt   \n",
       "5                  424B2_dp120183_424b2-302_N.txt   \n",
       "6                        424B2_g110203424b2_N.txt   \n",
       "7                            424B2_gs-424b2_N.txt   \n",
       "8            424B3_pricingsupplement2-yrfrn_N.txt   \n",
       "9                       424B5_d832846d424b5_C.txt   \n",
       "10                      424B5_d834717d424b5_C.txt   \n",
       "11                      424B5_d844216d424b5_C.txt   \n",
       "\n",
       "                                             contents  \n",
       "0   We may, at our option, redeem the floating rat...  \n",
       "1   We may redeem any series of the notes at our o...  \n",
       "2   CALCULATION OF REGISTRATION FEE Title of each ...  \n",
       "3   Citigroup Global Markets Holdings Inc. Novembe...  \n",
       "4   Citigroup Global Markets Holdings Inc. Novembe...  \n",
       "5   Citigroup Global Markets Holdings Inc. January...  \n",
       "6   Royal Bank of Canada Filed Pursuant to Rule 42...  \n",
       "7   gs-424b2.htm Filed Pursuant to Rule 424(b)(2) ...  \n",
       "8   Document CALCULATION OF REGISTRATION FEE Title...  \n",
       "9   If at any time the Bank shall become obligated...  \n",
       "10  We may redeem the notes at our option, in whol...  \n",
       "11  Unless we redeem the notes earlier, the notes ...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data=[]\n",
    "path = \"data/Train/\"\n",
    "\n",
    "for filename in sorted(os.listdir('data/Train')):\n",
    "    if filename.endswith('.txt'):\n",
    "        xpath = os.path.join(path,filename)\n",
    "        with open(xpath, 'r') as file:\n",
    "            contents = file.read()\n",
    "            data.append((filename,contents))\n",
    "filedf = pd.DataFrame(data, columns=['filename','contents'])\n",
    "filedf['callable'] = ''\n",
    "filedf = filedf[['callable', 'filename', 'contents']]\n",
    "filedf.to_csv('data/train.csv')\n",
    "filedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "path = 'data/Train'\n",
    "arr = os.listdir(path)\n",
    "data = []\n",
    "filenames = []\n",
    "for filename in arr:\n",
    "    if filename.endswith('.txt'):\n",
    "        xpath = os.path.join(path,filename)\n",
    "        with open(xpath, 'r') as file:\n",
    "            contents = file.read()\n",
    "            data.append(tokenizer.tokenize(contents))\n",
    "            filenames.append(filename)\n",
    "training = pd.DataFrame({'filename':filenames, 'data': data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>[We may, at our option, redeem the floating ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>424B2_d859033d424b2_N.txt</td>\n",
       "      <td>[We may redeem any series of the notes at our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424B2_dp114411_424b2-mtn1197_N.txt</td>\n",
       "      <td>[CALCULATION OF REGISTRATION FEE Title of each...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>424B2_dp115966_424b2-255_N.txt</td>\n",
       "      <td>[Citigroup Global Markets Holdings Inc., Novem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>424B2_dp116691_424b2-257_N.txt</td>\n",
       "      <td>[Citigroup Global Markets Holdings Inc., Novem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>424B2_dp120183_424b2-302_N.txt</td>\n",
       "      <td>[Citigroup Global Markets Holdings Inc., Janua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>424B2_g110203424b2_N.txt</td>\n",
       "      <td>[Royal Bank of Canada Filed Pursuant to Rule 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>424B2_gs-424b2_N.txt</td>\n",
       "      <td>[gs-424b2.htm Filed Pursuant to Rule 424(b)(2)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>424B3_pricingsupplement2-yrfrn_N.txt</td>\n",
       "      <td>[Document CALCULATION OF REGISTRATION FEE Titl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>424B5_d832846d424b5_C.txt</td>\n",
       "      <td>[If at any time the Bank shall become obligate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>424B5_d834717d424b5_C.txt</td>\n",
       "      <td>[We may redeem the notes at our option, in who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>[Unless we redeem the notes earlier, the notes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  \\\n",
       "0              424B2_d833231d424b2_C.txt   \n",
       "1              424B2_d859033d424b2_N.txt   \n",
       "2     424B2_dp114411_424b2-mtn1197_N.txt   \n",
       "3         424B2_dp115966_424b2-255_N.txt   \n",
       "4         424B2_dp116691_424b2-257_N.txt   \n",
       "5         424B2_dp120183_424b2-302_N.txt   \n",
       "6               424B2_g110203424b2_N.txt   \n",
       "7                   424B2_gs-424b2_N.txt   \n",
       "8   424B3_pricingsupplement2-yrfrn_N.txt   \n",
       "9              424B5_d832846d424b5_C.txt   \n",
       "10             424B5_d834717d424b5_C.txt   \n",
       "11             424B5_d844216d424b5_C.txt   \n",
       "\n",
       "                                                 data  \n",
       "0   [We may, at our option, redeem the floating ra...  \n",
       "1   [We may redeem any series of the notes at our ...  \n",
       "2   [CALCULATION OF REGISTRATION FEE Title of each...  \n",
       "3   [Citigroup Global Markets Holdings Inc., Novem...  \n",
       "4   [Citigroup Global Markets Holdings Inc., Novem...  \n",
       "5   [Citigroup Global Markets Holdings Inc., Janua...  \n",
       "6   [Royal Bank of Canada Filed Pursuant to Rule 4...  \n",
       "7   [gs-424b2.htm Filed Pursuant to Rule 424(b)(2)...  \n",
       "8   [Document CALCULATION OF REGISTRATION FEE Titl...  \n",
       "9   [If at any time the Bank shall become obligate...  \n",
       "10  [We may redeem the notes at our option, in who...  \n",
       "11  [Unless we redeem the notes earlier, the notes...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training.explode('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.to_csv('data/training.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>data</th>\n",
       "      <th>callable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>We may, at our option, redeem the floating rat...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>For the avoidance of doubt, if the floating ra...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>Further, if fewer than all of the floating rat...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>In addition to the optional make-whole redempt...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>For the avoidance of doubt, if the fixed/float...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2739</th>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>The relevant prospectus supplement will specif...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>Any notice of redemption of debt securities of...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>In the case of a partial redemption, the trust...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2742</th>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>We or any of our subsidiaries may at any time ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>Any debt securities of any such series purchas...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2744 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  \\\n",
       "0     424B2_d833231d424b2_C.txt   \n",
       "1     424B2_d833231d424b2_C.txt   \n",
       "2     424B2_d833231d424b2_C.txt   \n",
       "3     424B2_d833231d424b2_C.txt   \n",
       "4     424B2_d833231d424b2_C.txt   \n",
       "...                         ...   \n",
       "2739  424B5_d844216d424b5_C.txt   \n",
       "2740  424B5_d844216d424b5_C.txt   \n",
       "2741  424B5_d844216d424b5_C.txt   \n",
       "2742  424B5_d844216d424b5_C.txt   \n",
       "2743  424B5_d844216d424b5_C.txt   \n",
       "\n",
       "                                                   data  callable  \n",
       "0     We may, at our option, redeem the floating rat...      True  \n",
       "1     For the avoidance of doubt, if the floating ra...      True  \n",
       "2     Further, if fewer than all of the floating rat...      True  \n",
       "3     In addition to the optional make-whole redempt...      True  \n",
       "4     For the avoidance of doubt, if the fixed/float...      True  \n",
       "...                                                 ...       ...  \n",
       "2739  The relevant prospectus supplement will specif...      True  \n",
       "2740  Any notice of redemption of debt securities of...      True  \n",
       "2741  In the case of a partial redemption, the trust...      True  \n",
       "2742  We or any of our subsidiaries may at any time ...      True  \n",
       "2743  Any debt securities of any such series purchas...      True  \n",
       "\n",
       "[2744 rows x 3 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = pd.read_csv('data/Training.csv')\n",
    "training['callable']= training['filename'].str.contains('C.txt') \n",
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PunktSentenceTokenizer()\n",
    "path = 'data/Test'\n",
    "arr = os.listdir(path)\n",
    "data = []\n",
    "filenames = []\n",
    "for filename in arr:\n",
    "    if filename.endswith('.txt'):\n",
    "        xpath = os.path.join(path,filename)\n",
    "        with open(xpath, 'r') as file:\n",
    "            contents = file.read()\n",
    "            data.append(tokenizer.tokenize(contents))\n",
    "            filenames.append(filename)\n",
    "test = pd.DataFrame({'filename':filenames, 'data': data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>424B2_a19-18449_4424b2_N.txt</td>\n",
       "      <td>[PROSPECTUS and PRICING SUPPLEMENT NO., 34 PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>424B2_a19-18449_5424b2.txt</td>\n",
       "      <td>[PROSPECTUS and PRICING SUPPLEMENT NO., 35 PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424B2_a19-18449_6424b2.txt</td>\n",
       "      <td>[PROSPECTUS and PRICING SUPPLEMENT NO., 36 PRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>424B2_a19-18959_1424b2.txt</td>\n",
       "      <td>[PROSPECTUS Dated March 14, 2018 and PRICING S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>424B2_a20-1355_3424b2.txt</td>\n",
       "      <td>[PROSPECTUS and PROSPECTUS SUPPLEMENT, each Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>424B2_a20-1355_4424b2.txt</td>\n",
       "      <td>[PROSPECTUS and PROSPECTUS SUPPLEMENT, each Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>424B2_d26200424b2.txt</td>\n",
       "      <td>[Royal Bank of Canada Filed Pursuant to Rule 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>424B2_d795231d424b2_C.txt</td>\n",
       "      <td>[424B2 Table of Contents This filing is made p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>424B2_d816260d424b2_N.txt</td>\n",
       "      <td>[Pricing Supplement Table of Contents This fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>424B2_d881651d424b2_C.txt</td>\n",
       "      <td>[424B2 Table of Contents CALCULATION OF REGIST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>424B2_dp120926_424b2-k1431.txt</td>\n",
       "      <td>[Pricing Supplement No., K1431 To the Underlyi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>424B2_dp121287_424b2-mtnps.txt</td>\n",
       "      <td>[CALCULATION OF REGISTRATION FEE Title of each...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>424B2_efc20-74_424b2.txt</td>\n",
       "      <td>[PROfilePageNumberReset%Num%1%PS-%% CALCULATIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>424B2_j1022190424b2_N.txt</td>\n",
       "      <td>[Royal Bank of Canada Filed Pursuant to Rule 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>424B5_a2240370z424b5_C.txt</td>\n",
       "      <td>[Use these links to rapidly review the documen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  \\\n",
       "0     424B2_a19-18449_4424b2_N.txt   \n",
       "1       424B2_a19-18449_5424b2.txt   \n",
       "2       424B2_a19-18449_6424b2.txt   \n",
       "3       424B2_a19-18959_1424b2.txt   \n",
       "4        424B2_a20-1355_3424b2.txt   \n",
       "5        424B2_a20-1355_4424b2.txt   \n",
       "6            424B2_d26200424b2.txt   \n",
       "7        424B2_d795231d424b2_C.txt   \n",
       "8        424B2_d816260d424b2_N.txt   \n",
       "9        424B2_d881651d424b2_C.txt   \n",
       "10  424B2_dp120926_424b2-k1431.txt   \n",
       "11  424B2_dp121287_424b2-mtnps.txt   \n",
       "12        424B2_efc20-74_424b2.txt   \n",
       "13       424B2_j1022190424b2_N.txt   \n",
       "14      424B5_a2240370z424b5_C.txt   \n",
       "\n",
       "                                                 data  \n",
       "0   [PROSPECTUS and PRICING SUPPLEMENT NO., 34 PRO...  \n",
       "1   [PROSPECTUS and PRICING SUPPLEMENT NO., 35 PRO...  \n",
       "2   [PROSPECTUS and PRICING SUPPLEMENT NO., 36 PRO...  \n",
       "3   [PROSPECTUS Dated March 14, 2018 and PRICING S...  \n",
       "4   [PROSPECTUS and PROSPECTUS SUPPLEMENT, each Da...  \n",
       "5   [PROSPECTUS and PROSPECTUS SUPPLEMENT, each Da...  \n",
       "6   [Royal Bank of Canada Filed Pursuant to Rule 4...  \n",
       "7   [424B2 Table of Contents This filing is made p...  \n",
       "8   [Pricing Supplement Table of Contents This fil...  \n",
       "9   [424B2 Table of Contents CALCULATION OF REGIST...  \n",
       "10  [Pricing Supplement No., K1431 To the Underlyi...  \n",
       "11  [CALCULATION OF REGISTRATION FEE Title of each...  \n",
       "12  [PROfilePageNumberReset%Num%1%PS-%% CALCULATIO...  \n",
       "13  [Royal Bank of Canada Filed Pursuant to Rule 4...  \n",
       "14  [Use these links to rapidly review the documen...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.explode('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('data/Test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>data</th>\n",
       "      <th>probability</th>\n",
       "      <th>callable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>424B2_a19-18449_4424b2_N.txt</td>\n",
       "      <td>PROSPECTUS and PRICING SUPPLEMENT NO.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>424B2_a19-18449_4424b2_N.txt</td>\n",
       "      <td>34 PROSPECTUS SUPPLEMENT, each Dated September...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424B2_a19-18449_4424b2_N.txt</td>\n",
       "      <td>333-217193 by Supplement No.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>424B2_a19-18449_4424b2_N.txt</td>\n",
       "      <td>1 dated June 27, 2018 and Filed Pursuant to Ru...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>424B2_a19-18449_4424b2_N.txt</td>\n",
       "      <td>2 dated January 4, 2019 U.S.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4554</th>\n",
       "      <td>424B5_a2240370z424b5_C.txt</td>\n",
       "      <td>LEGAL OPINIONS Cleary Gottlieb Steen &amp; Hamilto...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>424B5_a2240370z424b5_C.txt</td>\n",
       "      <td>counsel, and Slaughter and May, our English so...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4556</th>\n",
       "      <td>424B5_a2240370z424b5_C.txt</td>\n",
       "      <td>EXPERTS The financial statements incorporated ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4557</th>\n",
       "      <td>424B5_a2240370z424b5_C.txt</td>\n",
       "      <td>40 ZEQ.=4,SEQ=67,EFW=\"2240411\",CP=\"SANTANDER U...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>424B5_a2240370z424b5_C.txt</td>\n",
       "      <td>;5',USER='CHE108057',CD='30-DEC-2019;14:23' TH...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4559 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  \\\n",
       "0     424B2_a19-18449_4424b2_N.txt   \n",
       "1     424B2_a19-18449_4424b2_N.txt   \n",
       "2     424B2_a19-18449_4424b2_N.txt   \n",
       "3     424B2_a19-18449_4424b2_N.txt   \n",
       "4     424B2_a19-18449_4424b2_N.txt   \n",
       "...                            ...   \n",
       "4554    424B5_a2240370z424b5_C.txt   \n",
       "4555    424B5_a2240370z424b5_C.txt   \n",
       "4556    424B5_a2240370z424b5_C.txt   \n",
       "4557    424B5_a2240370z424b5_C.txt   \n",
       "4558    424B5_a2240370z424b5_C.txt   \n",
       "\n",
       "                                                   data probability callable  \n",
       "0                 PROSPECTUS and PRICING SUPPLEMENT NO.                       \n",
       "1     34 PROSPECTUS SUPPLEMENT, each Dated September...                       \n",
       "2                          333-217193 by Supplement No.                       \n",
       "3     1 dated June 27, 2018 and Filed Pursuant to Ru...                       \n",
       "4                          2 dated January 4, 2019 U.S.                       \n",
       "...                                                 ...         ...      ...  \n",
       "4554  LEGAL OPINIONS Cleary Gottlieb Steen & Hamilto...                       \n",
       "4555  counsel, and Slaughter and May, our English so...                       \n",
       "4556  EXPERTS The financial statements incorporated ...                       \n",
       "4557  40 ZEQ.=4,SEQ=67,EFW=\"2240411\",CP=\"SANTANDER U...                       \n",
       "4558  ;5',USER='CHE108057',CD='30-DEC-2019;14:23' TH...                       \n",
       "\n",
       "[4559 rows x 4 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('data/Test.csv')\n",
    "test['probability'] =\"\"\n",
    "test['callable']= \"\"\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English # Import the English language class\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation# Create our list of punctuation marks\n",
    "nlp = spacy.load('en') # Create list of stopwords\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "parser = English()# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "\n",
    "\n",
    "def spacy_tokenizer(sentence):# Creating tokenizer function\n",
    "    \n",
    "    mytokens = parser(sentence)# Creating token object, used to create documents with linguistic annotations.\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ] # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ] # Removing stop words\n",
    "    \n",
    "    return mytokens# return preprocessed list of tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize DataFrames (Not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Not used\n",
    "def computeTFDIF(tfBoow,idfs):\n",
    "    tfidf = {}\n",
    "    for sentence, val in tfBow.items():\n",
    "        tfdif(sentence) = val*idfs(sentence\n",
    "    return tfdif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "class RowIterator(TransformerMixin):\n",
    "    \"\"\" Prepare dataframe for DictVectorizer \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (row for _, row in X.iterrows())\n",
    "\n",
    "vectorizer = make_pipeline(RowIterator(), DictVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2744x2057 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2744 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can use vectorizer \n",
    "vectorizer.fit_transform(training[['data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4559x2057 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 412 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(test[['data']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize  and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>data</th>\n",
       "      <th>callable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>We may, at our option, redeem the floating rat...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>For the avoidance of doubt, if the floating ra...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>Further, if fewer than all of the floating rat...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>In addition to the optional make-whole redempt...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>424B2_d833231d424b2_C.txt</td>\n",
       "      <td>For the avoidance of doubt, if the fixed/float...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2739</th>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>The relevant prospectus supplement will specif...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>Any notice of redemption of debt securities of...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>In the case of a partial redemption, the trust...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2742</th>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>We or any of our subsidiaries may at any time ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>424B5_d844216d424b5_C.txt</td>\n",
       "      <td>Any debt securities of any such series purchas...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2744 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  \\\n",
       "0     424B2_d833231d424b2_C.txt   \n",
       "1     424B2_d833231d424b2_C.txt   \n",
       "2     424B2_d833231d424b2_C.txt   \n",
       "3     424B2_d833231d424b2_C.txt   \n",
       "4     424B2_d833231d424b2_C.txt   \n",
       "...                         ...   \n",
       "2739  424B5_d844216d424b5_C.txt   \n",
       "2740  424B5_d844216d424b5_C.txt   \n",
       "2741  424B5_d844216d424b5_C.txt   \n",
       "2742  424B5_d844216d424b5_C.txt   \n",
       "2743  424B5_d844216d424b5_C.txt   \n",
       "\n",
       "                                                   data  callable  \n",
       "0     We may, at our option, redeem the floating rat...      True  \n",
       "1     For the avoidance of doubt, if the floating ra...      True  \n",
       "2     Further, if fewer than all of the floating rat...      True  \n",
       "3     In addition to the optional make-whole redempt...      True  \n",
       "4     For the avoidance of doubt, if the fixed/float...      True  \n",
       "...                                                 ...       ...  \n",
       "2739  The relevant prospectus supplement will specif...      True  \n",
       "2740  Any notice of redemption of debt securities of...      True  \n",
       "2741  In the case of a partial redemption, the trust...      True  \n",
       "2742  We or any of our subsidiaries may at any time ...      True  \n",
       "2743  Any debt securities of any such series purchas...      True  \n",
       "\n",
       "[2744 rows x 3 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = pd.read_csv('data/Training.csv')\n",
    "training['callable']= training['filename'].str.contains('C.txt') \n",
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    2693\n",
       "True       51\n",
       "Name: callable, dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.callable.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English # Import the English language class\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x000002BEC3934DA0>),\n",
       "                ('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x000002BEC288EF28>,\n",
       "                                 vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([('cleaner', predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(training['data'],training['callable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model generation\n",
    "pipe.predict(training['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Predicting with a test dataset\n",
    "\n",
    "test['callable'] = pipe.predict_proba(test['data'])[:,1]\n",
    "Resolve = test.groupby('filename')['callable'].max() \n",
    "\n",
    "\n",
    "# Model Accuracy\n",
    "#print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "#print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "#print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename\n",
       "424B2_a19-18449_4424b2_N.txt      0.068964\n",
       "424B2_a19-18449_5424b2.txt        0.207525\n",
       "424B2_a19-18449_6424b2.txt        0.069939\n",
       "424B2_a19-18959_1424b2.txt        0.121834\n",
       "424B2_a20-1355_3424b2.txt         0.389613\n",
       "424B2_a20-1355_4424b2.txt         0.387714\n",
       "424B2_d26200424b2.txt             0.092872\n",
       "424B2_d795231d424b2_C.txt         1.000000\n",
       "424B2_d816260d424b2_N.txt         1.000000\n",
       "424B2_d881651d424b2_C.txt         1.000000\n",
       "424B2_dp120926_424b2-k1431.txt    0.161670\n",
       "424B2_dp121287_424b2-mtnps.txt    0.968512\n",
       "424B2_efc20-74_424b2.txt          1.000000\n",
       "424B2_j1022190424b2_N.txt         0.092872\n",
       "424B5_a2240370z424b5_C.txt        0.999999\n",
       "Name: callable, dtype: float64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>callable</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>424B2_a19-18449_4424b2_N.txt</th>\n",
       "      <td>0.068964</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_a19-18449_5424b2.txt</th>\n",
       "      <td>0.207525</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_a19-18449_6424b2.txt</th>\n",
       "      <td>0.069939</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_a19-18959_1424b2.txt</th>\n",
       "      <td>0.121834</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_a20-1355_3424b2.txt</th>\n",
       "      <td>0.389613</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_a20-1355_4424b2.txt</th>\n",
       "      <td>0.387714</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_d26200424b2.txt</th>\n",
       "      <td>0.092872</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_d795231d424b2_C.txt</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_d816260d424b2_N.txt</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_d881651d424b2_C.txt</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_dp120926_424b2-k1431.txt</th>\n",
       "      <td>0.161670</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_dp121287_424b2-mtnps.txt</th>\n",
       "      <td>0.968512</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_efc20-74_424b2.txt</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B2_j1022190424b2_N.txt</th>\n",
       "      <td>0.092872</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B5_a2240370z424b5_C.txt</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                callable probability\n",
       "filename                                            \n",
       "424B2_a19-18449_4424b2_N.txt    0.068964       False\n",
       "424B2_a19-18449_5424b2.txt      0.207525       False\n",
       "424B2_a19-18449_6424b2.txt      0.069939       False\n",
       "424B2_a19-18959_1424b2.txt      0.121834       False\n",
       "424B2_a20-1355_3424b2.txt       0.389613       False\n",
       "424B2_a20-1355_4424b2.txt       0.387714       False\n",
       "424B2_d26200424b2.txt           0.092872       False\n",
       "424B2_d795231d424b2_C.txt       1.000000        True\n",
       "424B2_d816260d424b2_N.txt       1.000000        True\n",
       "424B2_d881651d424b2_C.txt       1.000000        True\n",
       "424B2_dp120926_424b2-k1431.txt  0.161670       False\n",
       "424B2_dp121287_424b2-mtnps.txt  0.968512        True\n",
       "424B2_efc20-74_424b2.txt        1.000000        True\n",
       "424B2_j1022190424b2_N.txt       0.092872       False\n",
       "424B5_a2240370z424b5_C.txt      0.999999        True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call = pd.DataFrame(Resolve)\n",
    "call['probability'] = call['callable'].apply(lambda x: 'True' if x >= .75 else 'False')\n",
    "call"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
